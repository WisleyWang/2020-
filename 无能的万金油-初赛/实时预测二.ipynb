{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import requests\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, f1_score,mean_squared_error,explained_variance_score\n",
    "from scipy.stats import entropy, kurtosis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_pickle('anlydata/complect_2.pkl')\n",
    "data = pd.read_pickle('anlydata/samp_data.pkl')\n",
    "#读取相关字典\n",
    "carri_dict = np.load('anlydata/carri_dict.npy',allow_pickle=True).item()\n",
    "mmsi_dict = np.load('anlydata/mmsi_dict.npy',allow_pickle=True).item()\n",
    "trace_dict = np.load('anlydata/trace_dict.npy',allow_pickle=True).item()\n",
    "#data = reduce_mem(data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将表格中的值反编回字典trace_dict中的关键字\n",
    "d1 = pd.DataFrame.from_dict(trace_dict,orient='index',columns =['TRANSPORT_TRACE'])\n",
    "d1 = d1.reset_index().rename(columns={'index':'trace'})\n",
    "data = data.merge(d1,on='TRANSPORT_TRACE',how='left').reset_index(drop=True)\n",
    "del data['TRANSPORT_TRACE']\n",
    "data = data.rename(columns={'trace':'TRANSPORT_TRACE'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将表格中的值反编回字典mmsi_dict中的关键字\n",
    "d1 = pd.DataFrame.from_dict(mmsi_dict,orient='index',columns =['vesselMMSI'])\n",
    "d1 = d1.reset_index().rename(columns={'index':'mmsi'})\n",
    "data = data.merge(d1,on='vesselMMSI',how='left').reset_index(drop=True)\n",
    "del data['vesselMMSI']\n",
    "data = data.rename(columns={'mmsi':'vesselMMSI'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将表格中的值反编回字典carri_dict中的关键字\n",
    "d1 = pd.DataFrame.from_dict(carri_dict,orient='index',columns =['carrierName'])\n",
    "d1 = d1.reset_index().rename(columns={'index':'caname'})\n",
    "data = data.merge(d1,on='carrierName',how='left').reset_index(drop=True)\n",
    "del data['carrierName']\n",
    "data = data.rename(columns={'caname':'carrierName'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('train0523/testData 0626.csv')\n",
    "port = pd.read_csv('train0523/port_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.drop_duplicates([c for c in data.columns if c not in ['loadingOrder']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = data1.loc[data1['TRANSPORT_TRACE'].notnull()]\n",
    "data1['len'] = data1['TRANSPORT_TRACE'].str.split('-')\n",
    "data1['len'] = data1['len'].str.len()\n",
    "data1 = data1.loc[(data1['len']>=2)&(data1['len']<=3)]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取起始点和终点岗口\n",
    "def get_pot(df):\n",
    "    df['start_pot'] = df['TRANSPORT_TRACE'].str.split('-').apply(lambda x:x[0])\n",
    "    df['end_pot'] = df['TRANSPORT_TRACE'].str.split('-').apply(lambda x:x[-1])\n",
    "    return df\n",
    "train = get_pot(data1)\n",
    "del data1\n",
    "#train = reduce_mem(train)\n",
    "test = get_pot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test添加起始港口和终点港口的坐标\n",
    "port1 = port.rename(columns={'TRANS_NODE_NAME':'start_pot','LONGITUDE':\n",
    "                            'start_long','LATITUDE':'start_lat'})\n",
    "port2 = port.rename(columns={'TRANS_NODE_NAME':'end_pot','LONGITUDE':\n",
    "                            'end_long','LATITUDE':'end_lat'})\n",
    "test = test.merge(port1,on='start_pot',how='left')\n",
    "test = test.merge(port2,on='end_pot',how='left')\n",
    "del port1,port2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port1 = port.rename(columns={'TRANS_NODE_NAME':'start_pot','LONGITUDE':\n",
    "                            'start_long_1','LATITUDE':'start_lat_1'})\n",
    "port2 = port.rename(columns={'TRANS_NODE_NAME':'end_pot','LONGITUDE':\n",
    "                            'end_long_1','LATITUDE':'end_lat_1'})\n",
    "train = train.merge(port1,on='start_pot',how='left')\n",
    "train = train.merge(port2,on='end_pot',how='left')\n",
    "del port1,port2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train添加起始港口和终点港口的坐标\n",
    "tmp=train.drop_duplicates('loadingOrder',keep='last')\n",
    "tmp = tmp[['loadingOrder','longitude','latitude']].rename(columns={'longitude':\n",
    "                                            'end_long','latitude':'end_lat'})\n",
    "tmp1=train.drop_duplicates('loadingOrder',keep='first')\n",
    "tmp1 = tmp1[['loadingOrder','longitude','latitude']].rename(columns={'longitude':\n",
    "                                            'start_long','latitude':'start_lat'})\n",
    "train = train.merge(tmp,on='loadingOrder',how='left')\n",
    "train = train.merge(tmp1,on='loadingOrder',how='left')\n",
    "#train = reduce_mem(train)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.loc[train['start_long'].notnull()]\n",
    "train = train.loc[train['start_lat'].notnull()]\n",
    "train = train.loc[train['end_long'].notnull()]\n",
    "train = train.loc[train['end_lat'].notnull()]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.loc[(abs((train['start_long'] - train['start_long_1'])+(train['start_lat'] - train['start_lat_1']))<5)] \n",
    "train = train.loc[(abs((train['end_long'] - train['end_long_1'])+(train['end_lat'] - train['end_lat_1']))<5)]\n",
    "del train['start_long_1'],train['start_lat_1'],train['end_long_1'],train['end_lat_1']\n",
    "#train = reduce_mem(train)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, model='train'):\n",
    "    #转换成时间戳，并且将每个运单按照时间排序\n",
    "    assert model=='train' or model=='test'\n",
    "    data.sort_values(['loadingOrder','timestamp'],inplace=True)\n",
    "    if model=='train':\n",
    "        pass\n",
    "#         data['vesselNextportETA'] = pd.to_datetime(data['vesselNextportETA'], infer_datetime_format=True) \n",
    "    else:\n",
    "        data['onboardDate'] = pd.to_datetime(data['onboardDate'], infer_datetime_format=True)\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], infer_datetime_format=True)    \n",
    "    return data\n",
    "def get_anchor(df):\n",
    "    # 转化为360度数\n",
    "    df['direction']=df['direction'].values/10\n",
    "    tmp=df.groupby('loadingOrder')\n",
    "    df['lat_diff'] = tmp['latitude'].diff(1)\n",
    "    df['lon_diff'] = tmp['longitude'].diff(1)\n",
    "    df['lat_diff_1'] = tmp['latitude'].diff(-1)\n",
    "    df['lon_diff_1'] = tmp['longitude'].diff(-1)\n",
    "    df['speed_diff'] = tmp['speed'].diff(1)\n",
    "    df['speed_diff_1'] = tmp['speed'].diff(-1)\n",
    "    df['direction_diff']=tmp['direction'].diff(1)\n",
    "    df['diff_seconds'] = tmp['timestamp'].diff(1).dt.total_seconds()\n",
    "    ### 这样实际是做了一个采样！！ #可以去除重复的记录\n",
    "    df['anchor'] =((df['lat_diff']<= 0.03)&(df['lon_diff'] <= 0.03)&(df['speed_diff'] <= 0.3)).astype('int')\n",
    "    ###  这里标记下船几乎停止的地方\n",
    "    df['stop']=((df['lat_diff'] <= 0.03)&(df['lon_diff'] <= 0.03)&(df['speed'] <= 1)).astype('int')\n",
    "    df['delay']=(df['diff_seconds']>3000).astype('int')\n",
    "    return df\n",
    "def distance(LatA,LatB,LonA,LonB):\n",
    "    EARTH_RADIUS = 6378.137 # 千米\n",
    "    def rad(d):\n",
    "        return d * np.pi/ 180.0\n",
    "    s=0\n",
    "    radLatA = rad(LatA)\n",
    "    radLatB = rad(LatB)\n",
    "    a = radLatA-radLatB\n",
    "    b = rad(LonA)-rad(LonB)\n",
    "    s= 2 * np.arcsin(np.sqrt(np.power(np.sin(a / 2),2)+ np.cos(radLatA) * np.cos(radLatB)*np.power(np.sin(b / 2),2)))\n",
    "    s=s* EARTH_RADIUS\n",
    "    #  保留两位小数\n",
    "    s = np.round(s * 100)/100\n",
    "    s = s * 1000 # 转换成m\n",
    "    return s\n",
    "def get_feature(df,model='train'):\n",
    "     #计算移动方便后面计算轨迹长度 m\n",
    "    df['move_leng']=distance(df.latitude.values,df.groupby('loadingOrder')['latitude'\n",
    "                ].shift(1).values,df.longitude.values,df.groupby('loadingOrder')['longitude'].shift(1).values)  \n",
    "    #计算下之前的累计距离\n",
    "    df['cusum_distance'] = df.groupby('loadingOrder')['move_leng'].cumsum()\n",
    "    \n",
    "    #-----------\n",
    "    #df['cusum_direction'] = df.groupby('loadingOrder')['direction_diff'].expanding().mean().reset_index(drop=True)\n",
    "    #df['cusum_mean_speed'] = df.groupby('loadingOrder')['speed'].expanding().mean().reset_index(drop=True)\n",
    "    df['cusum_stop'] = df.groupby('loadingOrder')['stop'].cumsum()\n",
    "    df['cusum_speed']=df.groupby('loadingOrder')['speed'].rolling(window=5).mean().reset_index(drop=True)\n",
    "    #------------------------------------------------------\n",
    "    df['direction_valc']=df['direction_diff']/df['diff_seconds']#\n",
    "    df['mean_speed'] = df['move_leng']/(df['diff_seconds']+0.01)\n",
    "    # 瞬时加速度 m/s2\n",
    "    df['instant_acc']=df['mean_speed']/(df['diff_seconds']+0.01)\n",
    "    \n",
    "    #获取船航行经度和维度的行驶比例和总航行占比\n",
    "    df['long_gap'] = abs(df['end_long']-df['longitude'])\n",
    "    df['lat_gap'] = abs(df['end_lat']-df['latitude'])\n",
    "    df['start_long_gap'] = abs(df['start_long']-df['longitude'])\n",
    "    df['start_lat_gap'] = abs(df['start_lat']-df['latitude'])\n",
    "    df['start_long_ratio'] = abs(df['longitude']-df['start_long']) / abs(df['end_long']-df['start_long'])\n",
    "    df['start_lat_ratio'] = abs(df['latitude']-df['start_lat']) / abs(df['end_lat']-df['start_lat'])\n",
    "    df['end_long_ratio'] = abs(df['longitude']-df['end_long']) / abs(df['end_long']-df['start_long'])\n",
    "    df['end_lat_ratio'] = abs(df['latitude']-df['end_lat']) / abs(df['end_lat']-df['start_lat'])\n",
    "    #获取总差距\n",
    "    df['all_start_gap'] = df['start_long_gap'] + df['start_lat_gap']\n",
    "    df['all_start_ratio'] = df['all_start_gap'] / (abs(df['end_long']-df['start_long'])+abs(df['end_lat']-df['start_lat']))\n",
    "    df['all_end_gap'] = df['long_gap'] + df['lat_gap']\n",
    "    df['all_end_ratio'] = df['all_end_gap'] / (abs(df['end_long']-df['start_long'])+abs(df['end_lat']-df['start_lat']))\n",
    "    \n",
    "    #获取年月日等时间特征\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['day'] = df['timestamp'].dt.day\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['time'] = df['month'].astype(str)+'-'+df['day'].astype(str)\n",
    "    \n",
    "    ## 得到最早的时间\n",
    "    tmp=df.drop_duplicates('loadingOrder',keep='first').reset_index(drop=True)\n",
    "    tmp=tmp[['loadingOrder','timestamp','direction']]\n",
    "    tmp.columns=['loadingOrder','start_time','start_direction']\n",
    "    df=df.merge(tmp,on='loadingOrder',how='left')\n",
    "    df['have_run_time']=(df['timestamp']-df['start_time']).dt.total_seconds()//3600\n",
    "    df['distanc2taget']=distance(df.latitude.values,df.end_lat.values,df.longitude.values,df.end_long.values)\n",
    "    df['cusum_mean_speed'] = df['cusum_distance']/(df['have_run_time']+0.01)\n",
    "    # 瞬时加速度 m/s2\n",
    "    df['cusum_instant_acc']=df['cusum_mean_speed']/(df['have_run_time']+0.01)\n",
    "    return df\n",
    "def type_encoding(train_data,test_data):\n",
    "    ### ----对类别进行编码\n",
    "    for f in ['TRANSPORT_TRACE','carrierName','vesselMMSI','time']:\n",
    "        unique_set=set(train_data[f].unique().tolist()+test_data[f].unique().tolist())\n",
    "        unique_dict={ f:i for i,f in enumerate(unique_set)}\n",
    "        test_data[f]=test_data[f].map(unique_dict)\n",
    "        train_data[f]=train_data[f].map(unique_dict)\n",
    "        \n",
    "    # 港口名称编码\n",
    "    unique_set=set(train_data['start_pot'].unique().tolist()+test_data['start_pot'].unique().tolist()\n",
    "                  +train_data['end_pot'].unique().tolist()+test_data['end_pot'].unique().tolist())\n",
    "    unique_dict={ f:i for i,f in enumerate(unique_set)}\n",
    "    for f in ['start_pot','end_pot']:\n",
    "        test_data[f]=test_data[f].map(unique_dict)\n",
    "        train_data[f]=train_data[f].map(unique_dict)\n",
    "    return train_data,test_data\n",
    "def get_label(df):\n",
    "    tmp = df.groupby('loadingOrder')['timestamp'].agg({'time_max':'max'})\n",
    "    df = df.merge(tmp,on='loadingOrder',how='left')\n",
    "    df['label'] = (df['time_max'] - df['timestamp']).dt.total_seconds()//3600\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = get_data(train,model='train')\n",
    "train = get_anchor(train)\n",
    "train = get_feature(train)\n",
    "train = get_label(train)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_data(test,model='test')\n",
    "test = get_anchor(test)\n",
    "test = get_feature(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = get_sample(train)\n",
    "test = get_sample1(test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,test1 = type_encoding(train,test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['leng']=train.groupby('loadingOrder')['timestamp'].transform('count')\n",
    "train1 = train.loc[(train['leng']>2)]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['direction', 'latitude', 'longitude', 'speed', 'diff_seconds', 'TRANSPORT_TRACE', 'vesselMMSI', 'start_pot', 'end_pot',\n",
    " 'end_long', 'end_lat', 'start_long', 'start_lat', 'lat_diff', 'lon_diff', 'speed_diff', 'direction_diff', 'move_leng',\n",
    " 'cusum_distance', 'cusum_stop', 'mean_speed', 'instant_acc', 'long_gap', 'lat_gap', 'start_long_gap', 'start_lat_gap', \n",
    " 'all_start_gap', 'all_end_gap', 'start_direction', 'have_run_time', 'cusum_mean_speed', 'cusum_instant_acc']14.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['direction', 'latitude', 'longitude', 'speed', 'diff_seconds', 'TRANSPORT_TRACE', 'vesselMMSI', 'carrierName', 'start_pot', \n",
    " 'end_pot', 'end_long', 'end_lat', 'start_long', 'start_lat', 'lat_diff', 'lon_diff', 'speed_diff', 'direction_diff', 'anchor',\n",
    " 'stop', 'delay', 'move_leng', 'cusum_distance', 'cusum_stop', 'direction_valc', 'mean_speed', 'instant_acc', 'long_gap', \n",
    " 'lat_gap', 'start_long_gap', 'start_lat_gap', 'all_start_gap', 'all_end_gap', 'start_direction', 'have_run_time', \n",
    " 'cusum_mean_speed', 'cusum_instant_acc']14.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['direction', 'latitude', 'longitude', 'speed', 'diff_seconds', 'TRANSPORT_TRACE', 'vesselMMSI', 'carrierName', 'start_pot', 'end_pot', 'end_long', 'end_lat', 'start_long', 'start_lat', 'lat_diff', 'lon_diff', 'speed_diff', 'direction_diff', 'anchor', 'stop', 'delay', 'move_leng', 'cusum_distance', 'cusum_stop', 'direction_valc', 'mean_speed', 'instant_acc', 'long_gap', 'lat_gap', 'start_long_gap', 'start_lat_gap', 'all_start_gap', 'all_end_gap', 'start_direction', 'have_run_time', 'cusum_mean_speed', 'cusum_instant_acc', 'cusum_speed']\n",
      "38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [c for c in train.columns if c in ['direction', 'latitude', 'longitude', 'speed', 'diff_seconds', 'TRANSPORT_TRACE', 'vesselMMSI', 'carrierName', 'start_pot', \n",
    " 'end_pot', 'end_long', 'end_lat', 'start_long', 'start_lat', 'lat_diff', 'lon_diff', 'speed_diff', 'direction_diff', 'anchor',\n",
    " 'stop', 'delay', 'move_leng', 'cusum_distance', 'cusum_stop', 'direction_valc', 'mean_speed', 'instant_acc', 'long_gap', \n",
    " 'lat_gap', 'start_long_gap', 'start_lat_gap', 'all_start_gap', 'all_end_gap', 'start_direction', 'have_run_time', \n",
    " 'cusum_mean_speed', 'cusum_instant_acc','cusum_speed']]\n",
    "print(features)\n",
    "print(len(features))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's l2: 3956.7\n",
      "[200]\ttraining's l2: 2066.48\n",
      "[300]\ttraining's l2: 1417.67\n",
      "[400]\ttraining's l2: 1037.52\n",
      "[500]\ttraining's l2: 817.842\n",
      "[600]\ttraining's l2: 647.177\n",
      "[700]\ttraining's l2: 536.525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,explained_variance_score\n",
    "from sklearn.model_selection import KFold\n",
    "from  lightgbm.sklearn import LGBMRegressor\n",
    "def mse_score_eval(preds, valid):\n",
    "    labels = valid.get_label()\n",
    "    scores = mean_squared_error(y_true=labels, y_pred=preds)\n",
    "    return 'mse_score', scores, True\n",
    "\n",
    "def build_model(train_data, test, pred, label, seed=1099, is_shuffle=True):\n",
    "    train_pred = np.zeros((train_data.shape[0], ))\n",
    "    test_pred = np.zeros((test.shape[0], ))\n",
    "    n_splits = 5\n",
    "    # Kfold\n",
    "    fold = KFold(n_splits=n_splits, shuffle=is_shuffle, random_state=seed)\n",
    "    kf_way = fold.split(train_data[pred])\n",
    "    # params\n",
    "#     test_x=np.concatenate([test[pred].values,geohash_test],axis=1)\n",
    "    # train\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(kf_way, start=1):\n",
    "        train_x, train_y = train_data[pred].iloc[train_idx].values, train_data[label].iloc[train_idx]\n",
    "        valid_x, valid_y = train_data[pred].iloc[valid_idx].values, train_data[label].iloc[valid_idx]\n",
    "#         geohash_tr_x,geohash_val_x=geohash_train[train_idx],geohash_train[valid_idx]\n",
    "#         train_x=np.concatenate([train_x,geohash_tr_x],axis=1)\n",
    "#         valid_x=np.concatenate([valid_x,geohash_val_x],axis=1)\n",
    "        \n",
    "        # 数据加载\n",
    "        clf=LGBMRegressor( learning_rate=0.5,\n",
    "        n_estimators=4000,\n",
    "        boosting_type = 'gbdt',\n",
    "        objective = 'regression',\n",
    "        num_leaves=256,\n",
    "        subsample=0.8,\n",
    "        njobs=-1,\n",
    "        max_depth=6,\n",
    "        reg_lambda=0,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=2019,  # 2019\n",
    "        metric=['mse'])\n",
    "        \n",
    "        clf.fit(\n",
    "        train_x, train_y,\n",
    "        eval_set=[(valid_x, valid_y)],\n",
    "        eval_metric=['mse'],\n",
    "        categorical_feature='auto',\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=100)        \n",
    "        \n",
    "        train_pred[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration_)\n",
    "        \n",
    "        \n",
    "        test_pred += clf.predict(test[pred], num_iteration=clf.best_iteration_)/fold.n_splits\n",
    "    \n",
    "    print('mean_squared_error:',mean_squared_error(train_data[label].values,train_pred))\n",
    "    test['label'] = test_pred\n",
    "    return test[['loadingOrder', 'label']],clf\n",
    "\n",
    "\n",
    "def bulid_onetrain(train_data, test,pred= features,label= 'label',seed=1099, is_shuffle=True):\n",
    "    train_x,train_y=train_data[features].values,train_data[label].values\n",
    "    clf=LGBMRegressor( learning_rate=0.05,\n",
    "    boosting_type = 'gbdt',\n",
    "    objective = 'regression',\n",
    "    n_estimators=6000,\n",
    "    num_leaves=156,\n",
    "    subsample=0.8,\n",
    "    njobs=-1,\n",
    "    max_depth=6,\n",
    "    reg_lambda=0,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2019,  # 2019\n",
    "    metric=['mse'])\n",
    "\n",
    "    clf.fit(\n",
    "    train_x, train_y,\n",
    "    eval_set=[(train_x, train_y)],\n",
    "    eval_metric=['mse'],\n",
    "    categorical_feature='auto',\n",
    "    early_stopping_rounds=100,\n",
    "    verbose=100)        \n",
    "\n",
    "    #train_pred= clf.predict(train_x, num_iteration=clf.best_iteration_)\n",
    "\n",
    "\n",
    "    test_pred= clf.predict(test[pred], num_iteration=clf.best_iteration_)\n",
    "\n",
    "    #print('mean_squared_error:',mean_squared_error(train_y,train_pred))\n",
    "    test['label'] = test_pred\n",
    "    return test[['loadingOrder', 'label']],clf\n",
    "#result,clf = build_model(train1, test,pred= features,label= 'label', is_shuffle=True)\n",
    "result,clf=bulid_onetrain(train1, test1,pred= features,label= 'label',is_shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['onboardDate'] = pd.to_datetime(test['onboardDate'])\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "test['timestamp'] = test['timestamp'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "test['ETA']=(test['timestamp']+test['label'].apply(lambda x:pd.Timedelta(hours=x))).apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "#test.drop(['direction','TRANSPORT_TRACE'],axis=1,inplace=True)\n",
    "test['onboardDate'] = test['onboardDate'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "test['onboardDate'] = pd.to_datetime(test['onboardDate'])\n",
    "test['creatDate'] = pd.datetime.now().strftime('%Y/%m/%d  %H:%M:%S')\n",
    "#test['timestamp'] =test['timestamp'].apply(lambda x:x.strftime('%Y-%m-%dT%H:%M:%S.000Z'))\n",
    "test['sub_onboard'] = (test['timestamp']-test['onboardDate']).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test[['loadingOrder', 'timestamp', 'longitude', 'latitude', 'carrierName', 'vesselMMSI', 'onboardDate', 'ETA', 'creatDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测结果取均值\n",
    "result['base_time'] = '2019/01/01  00:00:00'\n",
    "result['base_time'] = pd.to_datetime(result['base_time'])\n",
    "result['ETA'] = pd.to_datetime(result['ETA'])\n",
    "result['time_gap'] = (result['ETA'] - result['base_time']).dt.total_seconds()\n",
    "result['gap'] = result.groupby('loadingOrder')['time_gap'].transform('mean')\n",
    "result['ETA1'] = (result['base_time']+result['gap'].apply(lambda x:pd.Timedelta(seconds=x))).apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result[['loadingOrder', 'timestamp', 'longitude', 'latitude', 'carrierName', 'vesselMMSI', 'onboardDate', 'ETA1', 'creatDate']].rename(columns={'ETA1':'ETA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_csv('B10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.752160636091265"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(((pd.to_datetime(result2.ETA)-pd.to_datetime(base.ETA)).dt.total_seconds().values//3600)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loadingOrder</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>carrierName</th>\n",
       "      <th>vesselMMSI</th>\n",
       "      <th>onboardDate</th>\n",
       "      <th>ETA</th>\n",
       "      <th>creatDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AE223035353902</td>\n",
       "      <td>2019-07-03T21:16:48.000Z</td>\n",
       "      <td>120.093858</td>\n",
       "      <td>22.581320</td>\n",
       "      <td>OIEQNT</td>\n",
       "      <td>C2075927370</td>\n",
       "      <td>2019/07/02  04:12:48</td>\n",
       "      <td>2019/07/26  15:35:46</td>\n",
       "      <td>2020/07/01  22:12:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AE223035353902</td>\n",
       "      <td>2019-07-03T21:34:48.000Z</td>\n",
       "      <td>120.035707</td>\n",
       "      <td>22.617522</td>\n",
       "      <td>OIEQNT</td>\n",
       "      <td>C2075927370</td>\n",
       "      <td>2019/07/02  04:12:48</td>\n",
       "      <td>2019/07/26  15:35:46</td>\n",
       "      <td>2020/07/01  22:12:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AE223035353902</td>\n",
       "      <td>2019-07-03T21:51:18.000Z</td>\n",
       "      <td>119.981800</td>\n",
       "      <td>22.658465</td>\n",
       "      <td>OIEQNT</td>\n",
       "      <td>C2075927370</td>\n",
       "      <td>2019/07/02  04:12:48</td>\n",
       "      <td>2019/07/26  15:35:46</td>\n",
       "      <td>2020/07/01  22:12:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AE223035353902</td>\n",
       "      <td>2019-07-03T21:54:18.000Z</td>\n",
       "      <td>119.970845</td>\n",
       "      <td>22.668688</td>\n",
       "      <td>OIEQNT</td>\n",
       "      <td>C2075927370</td>\n",
       "      <td>2019/07/02  04:12:48</td>\n",
       "      <td>2019/07/26  15:35:46</td>\n",
       "      <td>2020/07/01  22:12:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AE223035353902</td>\n",
       "      <td>2019-07-03T22:11:08.000Z</td>\n",
       "      <td>119.953628</td>\n",
       "      <td>22.756897</td>\n",
       "      <td>OIEQNT</td>\n",
       "      <td>C2075927370</td>\n",
       "      <td>2019/07/02  04:12:48</td>\n",
       "      <td>2019/07/26  15:35:46</td>\n",
       "      <td>2020/07/01  22:12:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34707</td>\n",
       "      <td>ZZ524449869421</td>\n",
       "      <td>2020-03-17T04:02:38.000Z</td>\n",
       "      <td>103.776707</td>\n",
       "      <td>1.252897</td>\n",
       "      <td>BHSOUA</td>\n",
       "      <td>P2595193878</td>\n",
       "      <td>2020/03/13  06:07:28</td>\n",
       "      <td>2020/04/05  02:53:03</td>\n",
       "      <td>2020/07/01  22:12:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34708</td>\n",
       "      <td>ZZ524449869421</td>\n",
       "      <td>2020-03-17T04:03:18.000Z</td>\n",
       "      <td>103.776312</td>\n",
       "      <td>1.253418</td>\n",
       "      <td>BHSOUA</td>\n",
       "      <td>P2595193878</td>\n",
       "      <td>2020/03/13  06:07:28</td>\n",
       "      <td>2020/04/05  02:53:03</td>\n",
       "      <td>2020/07/01  22:12:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34709</td>\n",
       "      <td>ZZ524449869421</td>\n",
       "      <td>2020-03-17T04:05:18.000Z</td>\n",
       "      <td>103.775175</td>\n",
       "      <td>1.254865</td>\n",
       "      <td>BHSOUA</td>\n",
       "      <td>P2595193878</td>\n",
       "      <td>2020/03/13  06:07:28</td>\n",
       "      <td>2020/04/05  02:53:03</td>\n",
       "      <td>2020/07/01  22:12:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34710</td>\n",
       "      <td>ZZ524449869421</td>\n",
       "      <td>2020-03-17T04:05:58.000Z</td>\n",
       "      <td>103.774803</td>\n",
       "      <td>1.255285</td>\n",
       "      <td>BHSOUA</td>\n",
       "      <td>P2595193878</td>\n",
       "      <td>2020/03/13  06:07:28</td>\n",
       "      <td>2020/04/05  02:53:03</td>\n",
       "      <td>2020/07/01  22:12:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34711</td>\n",
       "      <td>ZZ524449869421</td>\n",
       "      <td>2020-03-17T04:07:38.000Z</td>\n",
       "      <td>103.773883</td>\n",
       "      <td>1.256368</td>\n",
       "      <td>BHSOUA</td>\n",
       "      <td>P2595193878</td>\n",
       "      <td>2020/03/13  06:07:28</td>\n",
       "      <td>2020/04/05  02:53:03</td>\n",
       "      <td>2020/07/01  22:12:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34712 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loadingOrder                 timestamp   longitude   latitude  \\\n",
       "0      AE223035353902  2019-07-03T21:16:48.000Z  120.093858  22.581320   \n",
       "1      AE223035353902  2019-07-03T21:34:48.000Z  120.035707  22.617522   \n",
       "2      AE223035353902  2019-07-03T21:51:18.000Z  119.981800  22.658465   \n",
       "3      AE223035353902  2019-07-03T21:54:18.000Z  119.970845  22.668688   \n",
       "4      AE223035353902  2019-07-03T22:11:08.000Z  119.953628  22.756897   \n",
       "...               ...                       ...         ...        ...   \n",
       "34707  ZZ524449869421  2020-03-17T04:02:38.000Z  103.776707   1.252897   \n",
       "34708  ZZ524449869421  2020-03-17T04:03:18.000Z  103.776312   1.253418   \n",
       "34709  ZZ524449869421  2020-03-17T04:05:18.000Z  103.775175   1.254865   \n",
       "34710  ZZ524449869421  2020-03-17T04:05:58.000Z  103.774803   1.255285   \n",
       "34711  ZZ524449869421  2020-03-17T04:07:38.000Z  103.773883   1.256368   \n",
       "\n",
       "      carrierName   vesselMMSI           onboardDate                   ETA  \\\n",
       "0          OIEQNT  C2075927370  2019/07/02  04:12:48  2019/07/26  15:35:46   \n",
       "1          OIEQNT  C2075927370  2019/07/02  04:12:48  2019/07/26  15:35:46   \n",
       "2          OIEQNT  C2075927370  2019/07/02  04:12:48  2019/07/26  15:35:46   \n",
       "3          OIEQNT  C2075927370  2019/07/02  04:12:48  2019/07/26  15:35:46   \n",
       "4          OIEQNT  C2075927370  2019/07/02  04:12:48  2019/07/26  15:35:46   \n",
       "...           ...          ...                   ...                   ...   \n",
       "34707      BHSOUA  P2595193878  2020/03/13  06:07:28  2020/04/05  02:53:03   \n",
       "34708      BHSOUA  P2595193878  2020/03/13  06:07:28  2020/04/05  02:53:03   \n",
       "34709      BHSOUA  P2595193878  2020/03/13  06:07:28  2020/04/05  02:53:03   \n",
       "34710      BHSOUA  P2595193878  2020/03/13  06:07:28  2020/04/05  02:53:03   \n",
       "34711      BHSOUA  P2595193878  2020/03/13  06:07:28  2020/04/05  02:53:03   \n",
       "\n",
       "                  creatDate  \n",
       "0      2020/07/01  22:12:14  \n",
       "1      2020/07/01  22:12:14  \n",
       "2      2020/07/01  22:12:14  \n",
       "3      2020/07/01  22:12:14  \n",
       "4      2020/07/01  22:12:14  \n",
       "...                     ...  \n",
       "34707  2020/07/01  22:12:14  \n",
       "34708  2020/07/01  22:12:14  \n",
       "34709  2020/07/01  22:12:14  \n",
       "34710  2020/07/01  22:12:14  \n",
       "34711  2020/07/01  22:12:14  \n",
       "\n",
       "[34712 rows x 9 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = result[['loadingOrder','ETA']].drop_duplicates('loadingOrder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = pd.read_csv('train0523/testData 0626.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = test3.merge(result1,on='loadingOrder',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = test3[['loadingOrder', 'timestamp', 'longitude', 'latitude', 'carrierName', 'vesselMMSI', 'onboardDate', 'ETA', 'creatDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3['creatDate'] = pd.datetime.now().strftime('%Y/%m/%d  %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#添加终点港口的时间\n",
    "tmp=test2.drop_duplicates('loadingOrder',keep='last')\n",
    "tmp = tmp[['loadingOrder','timestamp']].rename(columns={'timestamp':\n",
    "                                            'timestamp1'})\n",
    "test2 = test2.merge(tmp,on='loadingOrder',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1['onboardDate'] = pd.to_datetime(B1['onboardDate'])\n",
    "B1['onboardDate'] = B1['onboardDate'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "B1['ETA'] = pd.to_datetime(B1['ETA'])\n",
    "B1['ETA'] = B1['ETA'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "B1['creatDate'] = pd.datetime.now().strftime('%Y/%m/%d  %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2['timestamp'] = pd.to_datetime(test2['timestamp'])\n",
    "test2['timestamp'] = test2['timestamp'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test2.merge(result,on=['vesselMMSI','onboardDate'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test[['loadingOrder', 'timestamp', 'longitude', 'latitude', 'carrierName', 'vesselMMSI', 'onboardDate', 'ETA', 'creatDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['base_time'] = '2019/01/01  00:00:00'\n",
    "result['base_time'] = pd.to_datetime(result['base_time'])\n",
    "result['ETA'] = pd.to_datetime(result['ETA'])\n",
    "result['time_gap'] = (result['ETA'] - result['base_time']).dt.total_seconds()\n",
    "result['gap'] = result.groupby('loadingOrder')['time_gap'].transform('mean')\n",
    "result['ETA1'] = (result['base_time']+result['gap'].apply(lambda x:pd.Timedelta(seconds=x))).apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多结果均值融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = pd.read_csv('result5.csv')\n",
    "result2 = pd.read_csv('change1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2['onboardDate'] = pd.to_datetime(result2['onboardDate'])\n",
    "result2['onboardDate'] = result2['onboardDate'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "result2['ETA'] = pd.to_datetime(result2['ETA'])\n",
    "result2['ETA'] = result2['ETA'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "result2['creatDate'] = pd.to_datetime(result2['creatDate'])\n",
    "result2['creatDate'] = result2['creatDate'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1['timestamp'] = pd.to_datetime(result1['timestamp'])\n",
    "result1['timestamp'] = result1['timestamp'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "result1['base_time'] = '2019/01/01  00:00:00'\n",
    "result1['base_time'] = pd.to_datetime(result1['base_time'])\n",
    "result1['ETA'] = pd.to_datetime(result1['ETA'])\n",
    "result1['time_gap'] = (result1['ETA'] - result1['base_time']).dt.total_seconds()\n",
    "result1['gap'] = result1.groupby('loadingOrder')['time_gap'].transform('median')\n",
    "result1['ETA1'] = (result1['base_time']+result1['gap'].apply(lambda x:pd.Timedelta(seconds=x))).apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2['timestamp'] = pd.to_datetime(result2['timestamp'])\n",
    "result2['timestamp'] = result2['timestamp'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "result2['base_time'] = '2019/01/01  00:00:00'\n",
    "result2['base_time'] = pd.to_datetime(result1['base_time'])\n",
    "result2['ETA'] = pd.to_datetime(result2['ETA'])\n",
    "result2['time_gap'] = (result2['ETA'] - result2['base_time']).dt.total_seconds()\n",
    "result2['gap'] = result2.groupby('loadingOrder')['time_gap'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#求均值\n",
    "result1['gap1'] = result2['gap']\n",
    "result1['gap2'] = (result1['gap'] + result1['gap1'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1['ETA2'] = (result1['base_time']+result1['gap2'].apply(lambda x:pd.Timedelta(seconds=x))).apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result[['loadingOrder', 'timestamp', 'longitude', 'latitude', 'carrierName', 'vesselMMSI', 'onboardDate', 'ETA1', 'creatDate']].rename(columns={'ETA1':'ETA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['ETA'] = pd.to_datetime(result['ETA'])\n",
    "result['ETA'] = result['ETA'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#转化格式\n",
    "result['carrierName'] = test3['carrierName']\n",
    "result['vesselMMSI'] = test3['vesselMMSI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result[['loadingOrder', 'timestamp', 'longitude', 'latitude', 'carrierName', 'vesselMMSI', 'onboardDate', 'ETA1', 'creatDate']].rename(columns={'ETA1':'ETA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['timestamp'] = pd.to_datetime(result['timestamp'])\n",
    "result['timestamp'] =result['timestamp'].apply(lambda x:x.strftime('%Y-%m-%dT%H:%M:%S.000Z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3['creatDate'] = pd.datetime.now().strftime('%Y/%m/%d  %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('have_run_time', 25738),\n",
       " ('lat_gap', 19945),\n",
       " ('long_gap', 19503),\n",
       " ('cusum_distance', 18154),\n",
       " ('vesselMMSI', 15338),\n",
       " ('start_long_gap', 14088),\n",
       " ('start_lat', 13775),\n",
       " ('start_long', 13485),\n",
       " ('start_lat_gap', 13342),\n",
       " ('end_lat', 12374),\n",
       " ('cusum_mean_speed', 12285),\n",
       " ('latitude', 11822),\n",
       " ('cusum_instant_acc', 11706),\n",
       " ('end_long', 11373),\n",
       " ('lat_diff', 11083),\n",
       " ('TRANSPORT_TRACE', 10643),\n",
       " ('mean_speed', 10521),\n",
       " ('longitude', 10402),\n",
       " ('lon_diff', 8642),\n",
       " ('speed', 7767),\n",
       " ('diff_seconds', 5406),\n",
       " ('instant_acc', 4888),\n",
       " ('speed_diff', 2315)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_dict={features[i]: f for i,f in enumerate(clf.feature_importances_)}\n",
    "# import_dict.update({ i:v for i,v in enumerate(clf.feature_importances_[60:])})\n",
    "sorted(import_dict.items(),key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#速度特征\n",
    "#位移特征\n",
    "#轨迹特征"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
