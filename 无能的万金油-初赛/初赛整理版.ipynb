{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重新整理代码整理代码 2020年6月30号 晚20：21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import geopandas\n",
    "import gc\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from shapely.geometry import Point\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "import gensim\n",
    "# plt.rcParams['font.sans-serif'] = ['KaiTi']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    gc.collect()\n",
    "    return df\n",
    "def get_data(data, model='train'):\n",
    "    assert model=='train' or model=='test'\n",
    "    data.sort_values(['loadingOrder','timestamp'],inplace=True)\n",
    "    if model=='train':\n",
    "        pass\n",
    "#         data['vesselNextportETA'] = pd.to_datetime(data['vesselNextportETA'], infer_datetime_format=True) \n",
    "    else:\n",
    "        data['onboardDate'] = pd.to_datetime(data['onboardDate'], infer_datetime_format=True)\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], infer_datetime_format=True)    \n",
    "    return data\n",
    "def get_anchor(df):\n",
    "    # 转化为360度数\n",
    "    df['direction']=df['direction']/100\n",
    "    df['cos_direction']=np.cos(df['direction']/180*np.pi)\n",
    "    \n",
    "#     df['direction1']=np.sin(df['direction']/180*np.pi)\n",
    "    tmp=df.groupby('loadingOrder')\n",
    "    df['lat_diff'] = tmp['latitude'].diff(1)\n",
    "    df['lon_diff'] = tmp['longitude'].diff(1)\n",
    "    df['speed_diff'] = tmp['speed'].diff(1)\n",
    "    df['direction_diff']=tmp['direction'].diff(1)\n",
    "    df['diff_seconds'] = tmp['timestamp'].diff(1).dt.total_seconds() \n",
    "    ### 这样实际是做了一个采样！！ #可以去除重复的记录\n",
    "    df['anchor'] =(((df['lat_diff']> 1)|(df['lon_diff'] > 1))&(df['speed']==0)).astype('int')\n",
    "    ###  这里标记下船几乎停止的地方\n",
    "    df['stop']=((df['lat_diff'] <= 0.03)&(df['lon_diff'] <= 0.03)&(df['speed'] <= 1)).astype('int')\n",
    "#     df['delay']=(df['diff_seconds']>3000).astype('int')\n",
    "    return df\n",
    "def distance(LatA,LatB,LonA,LonB):\n",
    "    EARTH_RADIUS = 6378.137 # 千米\n",
    "    def rad(d):\n",
    "        return d * np.pi/ 180.0\n",
    "    s=0\n",
    "    radLatA = rad(LatA)\n",
    "    radLatB = rad(LatB)\n",
    "    a = radLatA-radLatB\n",
    "    b = rad(LonA)-rad(LonB)\n",
    "    s= 2 * np.arcsin(np.sqrt(np.power(np.sin(a / 2),2)+ np.cos(radLatA) * np.cos(radLatB)*np.power(np.sin(b / 2),2)))\n",
    "    s=s* EARTH_RADIUS\n",
    "    #  保留两位小数\n",
    "    s = np.round(s * 100)/100\n",
    "    s = s * 1000 # 转换成m\n",
    "    return s\n",
    "def get_feature(df,model='train'):\n",
    "     #计算移动方便后面计算轨迹长度 m\n",
    "    df['move_leng']=distance(df.latitude.values,df.groupby('loadingOrder')['latitude'\n",
    "                ].shift(1).values,df.longitude.values,df.groupby('loadingOrder')['longitude'].shift(1).values)  \n",
    "    ### 计算下之前的累积距离：\n",
    "    df['cusum_distance']=df.groupby('loadingOrder')['move_leng'].expanding().sum().reset_index(drop=True)\n",
    "    df['cusum_speed']=df.groupby('loadingOrder')['speed'].rolling(window=5).mean().reset_index(drop=True)\n",
    "    \n",
    "    tmp=df.groupby('loadingOrder')['move_leng'].sum().reset_index()\n",
    "    tmp.columns=['loadingOrder','sum_distance']\n",
    "    df=df.merge(tmp,on='loadingOrder',how='left')\n",
    "    #-----\n",
    "#     df['cusum_direction']=df.groupby('loadingOrder')['direction_diff'].expanding().mean().reset_index(drop=True)\n",
    "#     df['cusum_stop']=df.groupby('loadingOrder')['stop'].expanding().sum().reset_index(drop=True)\n",
    "#     df['stop_time_gap']=df['cusum_stop']*df['diff_seconds']\n",
    "    \n",
    "    df['instant_speed']=df['move_leng']/df['diff_seconds']\n",
    "     # 瞬时加速度 m/s2\n",
    "    df['instant_acc']=df['instant_speed']/(df['diff_seconds']+0.01)\n",
    "#     df['instant_speed_change']=df['speed_diff']/df['diff_seconds']\n",
    "#     df['status_sum']=df['anchor']+df['stop']+df['delay']\n",
    "    ####---------\n",
    "#     df['speed_acc']=df['speed']/df['diff_seconds']*60\n",
    "    # 瞬时加速度 m/s2\n",
    "#     df['instant_acc']=df['instant_speed']/df['diff_seconds']\n",
    "#     df['direction_valc']=df['direction_diff']/df['diff_seconds']#\n",
    "    \n",
    "    # local\n",
    "#     df['local_lat_lon']=df['longitude'].astype('int')*10+df['latitude'].astype('int')\n",
    "    ## 得到最早的时间\n",
    "#     tmp=df.drop_duplicates('loadingOrder',keep='first').reset_index(drop=True)\n",
    "#     tmp=tmp[['loadingOrder','timestamp']]\n",
    "#     tmp.columns=['loadingOrder','start_time']\n",
    "#     df=df.merge(tmp,on='loadingOrder',how='left')\n",
    "#     df['have_run_time']=(df['timestamp']-df['start_time']).dt.total_seconds()/3600\n",
    "    df['have_run_time']=df.groupby('loadingOrder')['diff_seconds'].expanding().sum().reset_index(drop=True)\n",
    "    df['distanc2taget']=distance(df.latitude.values,df.end_latitude.values,df.longitude.values,df.end_longitude.values)\n",
    "    df['distanc2start']=distance(df.latitude.values,df.start_latitude.values,df.longitude.values,df.start_longitude.values)\n",
    "    df['curr_value_time']=(df['distanc2taget']/ df['instant_speed']+0.01)*3600\n",
    "    df['avg_speed']=df['cusum_distance']/df['have_run_time']*3600\n",
    "    #-----当前日期：\n",
    "    df['day_tag']=df['timestamp'].dt.year.values*10000+df['timestamp'].dt.month.values*100+df['timestamp'].dt.day.values\n",
    "    df['curr_week']=df['timestamp'].dt.week\n",
    "    df['curr_hour']=df['timestamp'].dt.hour\n",
    "    ###  除了描述当前状态 还要描述\n",
    "    ##-----\n",
    "#     df['long_gap'] = abs(df['end_longitude']-df['longitude'])\n",
    "#     df['lat_gap'] = abs(df['end_latitude']-df['latitude'])\n",
    "#     df['start_long_gap'] = abs(df['start_longitude']-df['longitude'])\n",
    "#     df['start_lat_gap'] = abs(df['start_latitude']-df['latitude'])\n",
    "#     df['start_long_ratio'] = abs(df['longitude']-df['start_longitude']) / abs(df['end_longitude']-df['start_longitude'])\n",
    "#     df['start_lat_ratio'] = abs(df['latitude']-df['start_latitude']) / abs(df['end_latitude']-df['start_latitude'])\n",
    "#     df['end_long_ratio'] = abs(df['longitude']-df['end_longitude']) / abs(df['end_longitude']-df['start_longitude'])\n",
    "#     df['end_lat_ratio'] = abs(df['latitude']-df['end_latitude']) / abs(df['end_latitude']-df['start_latitude'])   \n",
    "    return df\n",
    "def get_labe(df,feat='label'):\n",
    "    tmp= df.groupby('loadingOrder')['timestamp'].agg({'end_time':'max','mmin':'min'}).reset_index()\n",
    "    tmp['label']=(tmp['end_time'] - tmp['mmin']).dt.total_seconds()//3600\n",
    "#     del df['lat_lon_dis']\n",
    "    return tmp[['loadingOrder',feat]]\n",
    "def type_encoding(train_data,test_data):\n",
    "    ### ----对类别进行编码\n",
    "    for f in ['TRANSPORT_TRACE','carrierName','vesselMMSI']:\n",
    "        unique_set=set(train_data[f].unique().tolist()+test_data[f].unique().tolist())\n",
    "        unique_dict={ f:i for i,f in enumerate(unique_set)}\n",
    "        test_data[f]=test_data[f].map(unique_dict)\n",
    "        train_data[f]=train_data[f].map(unique_dict)\n",
    "        \n",
    "    # 港口名称编码\n",
    "    unique_set=set(train_data['start_sport'].unique().tolist()+test_data['start_sport'].unique().tolist()\n",
    "                  +train_data['end_sport'].unique().tolist()+test_data['end_sport'].unique().tolist())\n",
    "    unique_dict={ f:i for i,f in enumerate(unique_set)}\n",
    "    for f in ['start_sport','end_sport']:\n",
    "        test_data[f]=test_data[f].map(unique_dict)\n",
    "        train_data[f]=train_data[f].map(unique_dict)\n",
    "    return train_data,test_data\n",
    "\n",
    "def random_cut(x):\n",
    "    leng=len(x)\n",
    "    ## 尽量取中间 这里的起始位可以稍微修正一下\n",
    "    start=np.random.randint(0,2,leng)\n",
    "    start\n",
    "    return start.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理文件\n",
    "# 数据流读取\n",
    "（读取时间较长。。。。。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  这里位了核查所有数据 只取基本列信息，并去除重复的\n",
    "# names= ['loadingOrder','carrierName','timestamp','longitude',\n",
    "#                   'latitude','vesselMMSI','speed','direction','vesselNextport',\n",
    "#                   'vesselNextportETA','vesselStatus','vesselDatasource','TRANSPORT_TRACE']\n",
    "\n",
    "# train_flux=pd.read_csv('./train0523.csv',chunksize=500000,names=names)\n",
    "# train_df=pd.DataFrame(columns=names)\n",
    "# for i,data in tqdm(enumerate(train_flux)):\n",
    "#     del data['vesselDatasource'],data['vesselNextport'],data['vesselNextportETA'],data['vesselStatus']\n",
    "#     data.drop_duplicates(['loadingOrder','timestamp','carrierName'],inplace=True)# 可能一艘船有好几家运输公司\n",
    "#     data=reduce_mem_usage(data) \n",
    "#     data['direction']=data['direction'].values.astype('int32')\n",
    "#     train_df=train_df.append(data)       \n",
    "# train_df=reduce_mem_usage(train_df)\n",
    "# train_df['speed']=train_df.speed.values.astype('int8')\n",
    "# train_df['direction']=train_df.direction.values.astype('int32')\n",
    "# train_df['timestamp']=pd.to_datetime(train_df.timestamp)\n",
    "# train_df.drop_duplicates(['loadingOrder','timestamp','carrierName'],inplace=True)\n",
    "\n",
    "# train_df.sort_values(['loadingOrder','timestamp'],inplace=True)\n",
    "# print(train_df.shape)\n",
    "# train_df.drop_duplicates(['loadingOrder','timestamp','longitude','latitude'],keep='last',inplace=True)\n",
    "# print(train_df.shape)\n",
    "\n",
    "\n",
    "#---\n",
    "# # 去除trace为nan的值\n",
    "# print(train_df.shape)\n",
    "# train_df=train_df.loc[train_df.TRANSPORT_TRACE.notna()]\n",
    "# print(train_df.shape)\n",
    "\n",
    "# train_df=pd.to_pickle('./data/complect_data2.pkl') # 保存处理文件\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总共有90艘船\n",
      "228个快递运单\n",
      "11个运货公司\n",
      "21条运输路径\n",
      "运输路段长度：[2 3]\n",
      "运输过程中的 spped 情况：[24 25 32 33 36 37 40 42 43 41 39 38 27 26 20 22 23 21 18 30  0  8 10 11\n",
      " 12 17 15 13  1  2  6 29 35 28  7  3 14 31 34 16 19  9  5  4 44 46 47 50\n",
      " 49]\n",
      "经度跨越：359.103702 纬度跨越：85.677953\n",
      "测试集中，船只的运输的港口：\n",
      "CNYTN-MXZLO          18420\n",
      "CNYTN-PAONX           4244\n",
      "CNSHK-MYTPP           2416\n",
      "CNSHK-CLVAP           2014\n",
      "CNYTN-ARENA           1574\n",
      "CNNSA-GHTEM           1358\n",
      "CNSHK-SGSIN           1120\n",
      "CNNSA-NAWVB            846\n",
      "CNYTN-MATNG            818\n",
      "CNYTN-CAVAN            731\n",
      "CNSHK-GRPIR            451\n",
      "CNSHK-INMUN            209\n",
      "HONGKONG-BU            139\n",
      "HKHKG-FRFOS             89\n",
      "CNHKG-ARBUE             80\n",
      "CNYTN-MTMLA             69\n",
      "CNSHK-PKQCT             60\n",
      "CNSHK-SIKOP             48\n",
      "CNSHK-SGSIN-AEJEA       10\n",
      "CNSHK-MYPKG             10\n",
      "CNYTN-MYTPP              6\n",
      "Name: TRANSPORT_TRACE, dtype: int64\n",
      "测试集时间跨度:\n",
      "min time:2019-01-16T17:01:48.000Z max time:2020-03-27T03:48:28.000Z\n"
     ]
    }
   ],
   "source": [
    "# 查看测试集状态\n",
    "test_df=pd.read_csv('./data/testData 0626.csv')\n",
    "print('总共有{}艘船'.format(test_df.vesselMMSI.nunique()))\n",
    "print('{}个快递运单'.format(test_df.loadingOrder.nunique()))\n",
    "print('{}个运货公司'.format(test_df.carrierName.nunique()))\n",
    "print('{}条运输路径'.format(test_df.TRANSPORT_TRACE.nunique()))\n",
    "print('运输路段长度：{}'.format(test_df.TRANSPORT_TRACE.apply(lambda x:len(x.split('-'))).unique()))\n",
    "print('运输过程中的 spped 情况：{}'.format(test_df.speed.unique()))\n",
    "print('经度跨越：{}'.format(test_df.longitude.max()-test_df.longitude.min()),'纬度跨越：{}'.format(test_df.latitude.max()-test_df.latitude.min()))\n",
    "print('测试集中，船只的运输的港口：')\n",
    "print(test_df.TRANSPORT_TRACE.value_counts())\n",
    "print('测试集时间跨度:')\n",
    "print('min time:{} max time:{}'.format(test_df.timestamp.min(),test_df.timestamp.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 清洗训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38382156, 10)\n",
      "(38381781, 10)\n",
      "193\n",
      "(6060, 2)\n",
      "(3337, 2)\n",
      "选取 loadingOrder： 3337\n",
      "清洗前： (38381781, 9)\n",
      "清洗后的数据： (21665825, 9)\n",
      "3337\n",
      "2896\n",
      "2896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:127: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n"
     ]
    }
   ],
   "source": [
    "train_df=pd.read_pickle('./data/sample_complect_data_2.pkl')\n",
    "train_df.sort_values(['loadingOrder','timestamp'],inplace=True)\n",
    "train_df['leng']=train_df.groupby('loadingOrder')['timestamp'].transform('count')\n",
    "# 必须保留起初speed==0的数据，因为test可能中间截取的，不过中间截取也没关系\n",
    "#我只需要在tran里面的label 也是从speed非0处开始计算，到末尾非0处非0即可\n",
    "# 去除稀疏的数据---\n",
    "print(train_df.shape)\n",
    "train_df=train_df.loc[train_df.leng>3]\n",
    "print(train_df.shape)\n",
    "del train_df['leng']\n",
    "\n",
    "#--------------\n",
    "# 加载港口坐标信息\n",
    "event=pd.read_csv('./data/port_2.csv')\n",
    "event=event[['TRANS_NODE_NAME','LONGITUDE','LATITUDE']]\n",
    "print(event.TRANS_NODE_NAME.nunique())\n",
    "# event=event.loc[~((event.LATITUDE<1)&(event.LONGITUDE==-0.386592))]\n",
    "# print(event.TRANS_NODE_NAME.nunique())\n",
    "\n",
    "#  计算起始和终止时刻经纬度误差\n",
    "end_trace=train_df.drop_duplicates('loadingOrder','last')\n",
    "end_trace=end_trace[['loadingOrder','TRANSPORT_TRACE','latitude','longitude']]\n",
    "end_trace['TRANS_NODE_NAME']=end_trace['TRANSPORT_TRACE'].apply(lambda x :x.split('-')[-1])\n",
    "end_trace=end_trace.merge(event,on='TRANS_NODE_NAME',how='left')\n",
    "end_trace['end_error']=np.abs((end_trace['longitude']-end_trace['LONGITUDE']).values)+np.abs((end_trace['latitude']-end_trace['LATITUDE']).values)\n",
    "end_trace=end_trace.loc[end_trace.end_error<5].reset_index(drop=True)\n",
    "#-----------------------------------------\n",
    "start_trace=train_df.drop_duplicates('loadingOrder','first')\n",
    "start_trace=start_trace[['loadingOrder','TRANSPORT_TRACE','latitude','longitude']]\n",
    "start_trace['TRANS_NODE_NAME']=start_trace['TRANSPORT_TRACE'].apply(lambda x :x.split('-')[0])\n",
    "start_trace=start_trace.merge(event,on='TRANS_NODE_NAME',how='left')\n",
    "start_trace['end_error']=np.abs((start_trace['longitude']-start_trace['LONGITUDE']).values)+np.abs((start_trace['latitude']-start_trace['LATITUDE']).values)\n",
    "start_trace=start_trace.loc[start_trace.end_error<5].reset_index(drop=True)\n",
    "#---------------------------------\n",
    "start_trace['tag']=1\n",
    "end_trace['tag']=1\n",
    "select_loading=pd.concat([start_trace,end_trace],axis=0)\n",
    "select_loading=select_loading.groupby('loadingOrder')['tag'].sum().reset_index()\n",
    "print(select_loading.shape)\n",
    "select_loading=select_loading.loc[select_loading.tag==2]\n",
    "print(select_loading.shape)\n",
    "del start_trace,end_trace\n",
    "\n",
    "print('选取 loadingOrder：',select_loading.loadingOrder.nunique())\n",
    "#--- 接下来就是从中选取我们需要的loadingOrder即可\n",
    "print('清洗前：',train_df.shape)\n",
    "train_df=train_df.loc[train_df.loadingOrder.isin(select_loading.loadingOrder)]\n",
    "print('清洗后的数据：',train_df.shape)\n",
    "del select_loading\n",
    "#  这里得保留end_trace,因为他是我们能获取的终点坐标的均值 可以后来给数据标记终点\n",
    "# 将speed转化下 否则计算差值可能有问题\n",
    "train_df['speed']=train_df['speed'].values.astype('int32')\n",
    "# del trace_dict,mmsi_dict,carri_dict,ni_carri_dict,ni_mmsi_dict,ni_trace_dict\n",
    "gc.collect()\n",
    "###--------------------\n",
    "## 计算统计特征 属于训练集特征统计\n",
    "# #首先 还是先划分起点和终点吧\n",
    "train_df['start_sport']=train_df['TRANSPORT_TRACE'].apply(lambda x:x.split('-')[0])\n",
    "#-----\n",
    "train_df['end_sport']=train_df['TRANSPORT_TRACE'].apply(lambda x:x.split('-')[-1])\n",
    "\n",
    "test_df=pd.read_csv('./data/testData 0626.csv')\n",
    "test_df['start_sport']=test_df['TRANSPORT_TRACE'].apply(lambda x:x.split('-')[0])\n",
    "test_df['end_sport']=test_df['TRANSPORT_TRACE'].apply(lambda x:x.split('-')[-1])\n",
    "print(train_df.loadingOrder.nunique())\n",
    "# train_df=train_df.loc[(train_df.end_sport.isin(test_df.end_sport))|(train_df.TRANSPORT_TRACE.isin(test_df.TRANSPORT_TRACE))].reset_index(drop=True)#(train_df.end_sport.isin(test_df.end_sport))|\n",
    "train_df['leng']=train_df.TRANSPORT_TRACE.apply(lambda x:len(x.split('-')))\n",
    "train_df=train_df.loc[(train_df.leng==3)|(train_df.leng==2)].reset_index(drop=True)\n",
    "# print(train_df.loadingOrder.nunique())\n",
    "#-------------------\n",
    "sport=pd.read_csv('./data/port_2.csv')\n",
    "sport=sport[['TRANS_NODE_NAME','LONGITUDE','LATITUDE']]\n",
    "sport.columns=['end_sport','end_longitude','end_latitude',]\n",
    "test_df=test_df.merge(sport,on='end_sport',how='left')\n",
    "\n",
    "sport.columns=['start_sport','start_longitude','start_latitude']\n",
    "test_df=test_df.merge(sport,on='start_sport',how='left')\n",
    "# \n",
    "sport=train_df.drop_duplicates('loadingOrder','last')[['loadingOrder','longitude','latitude']]\n",
    "# sport=sport.groupby('end_sport')[['longitude','latitude']].mean().reset_index()\n",
    "sport.columns=['loadingOrder','end_longitude','end_latitude',]\n",
    "train_df=train_df.merge(sport,on='loadingOrder',how='left')\n",
    "#\n",
    "sport=train_df.drop_duplicates('loadingOrder','first')[['loadingOrder','longitude','latitude']]\n",
    "# sport=sport.groupby('end_sport')[['longitude','latitude']].mean().reset_index()\n",
    "sport.columns=['loadingOrder','start_longitude','start_latitude',]\n",
    "train_df=train_df.merge(sport,on='loadingOrder',how='left')\n",
    "print(train_df.loadingOrder.nunique())\n",
    "\n",
    "train_df=train_df.loc[(train_df.end_latitude.notna())&(train_df.start_latitude.notna())]\n",
    "print(train_df.loadingOrder.nunique())\n",
    "\n",
    "# #------\n",
    "train_label=get_labe(train_df)\n",
    "#----\n",
    "train_df=train_df.merge(train_label,on='loadingOrder',how='left')\n",
    "# #----- 删除航行时间过短的\n",
    "# train_df=train_df.loc[train_df.label>20].reset_index(drop=True)\n",
    "\n",
    "#-------------对始发港统计label时间\n",
    "tmp=train_df.drop_duplicates('loadingOrder','last')\n",
    "\n",
    "start_sport_label=tmp.groupby('start_sport')['label'].mean().reset_index()\n",
    "start_sport_label.columns=['start_sport','start_mean_time'] \n",
    "# ----\n",
    "end_sport_label=tmp.groupby('end_sport')['label'].mean().reset_index()\n",
    "end_sport_label.columns=['end_sport','end_mean_time']\n",
    "#----\n",
    "trace_label=tmp.groupby('TRANSPORT_TRACE')['label'].mean().reset_index()\n",
    "trace_label.columns=['TRANSPORT_TRACE','TRANSPORT_TRACE_mean_time']\n",
    "## 还可以做 当前其实坐标与起始港坐标距离，因为test中有些并不是和起始港一致的\n",
    "del train_df['label']\n",
    "\n",
    "train_df=train_df.merge(start_sport_label,on='start_sport',how='left')\n",
    "train_df=train_df.merge(end_sport_label,on='end_sport',how='left')\n",
    "train_df=train_df.merge(trace_label,on='TRANSPORT_TRACE',how='left')\n",
    "#--------  ----- ----- ---- ------\n",
    "\n",
    "test_df=test_df.merge(start_sport_label,on='start_sport',how='left')\n",
    "test_df=test_df.merge(end_sport_label,on='end_sport',how='left')\n",
    "test_df=test_df.merge(trace_label,on='TRANSPORT_TRACE',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=get_data(train_df,model='train')# 转换下时间并排序\n",
    "train_df=get_anchor(train_df)\n",
    "test_df=get_data(test_df,model='test')\n",
    "test_df=get_anchor(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5480900, 49)\n",
      "(4559407, 49)\n"
     ]
    }
   ],
   "source": [
    "# 做一下降采样 和去除噪声数据\n",
    "print(train_df.shape)\n",
    "train_df=train_df.loc[train_df.diff_seconds>=180].reset_index(drop=True)\n",
    "print(train_df.shape)\n",
    "# train_df=train_df.loc[train_df.direction!=-1].reset_index(drop=True)\n",
    "# print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(df,model='train'):\n",
    "     #计算移动方便后面计算轨迹长度 m\n",
    "    df['move_leng']=distance(df.latitude.values,df.groupby('loadingOrder')['latitude'\n",
    "                ].shift(1).values,df.longitude.values,df.groupby('loadingOrder')['longitude'].shift(1).values)  \n",
    "    #计算下之前的累计距离\n",
    "    df['cusum_distance'] = df.groupby('loadingOrder')['move_leng'].cumsum()\n",
    "    \n",
    "    #-----------\n",
    "    #df['cusum_direction'] = df.groupby('loadingOrder')['direction_diff'].expanding().mean().reset_index(drop=True)\n",
    "    #df['cusum_mean_speed'] = df.groupby('loadingOrder')['speed'].expanding().mean().reset_index(drop=True)\n",
    "    df['cusum_stop'] = df.groupby('loadingOrder')['stop'].cumsum()\n",
    "    #------------------------------------------------------\n",
    "    df['direction_valc']=df['direction_diff']/df['diff_seconds']#\n",
    "    df['mean_speed'] = df['move_leng']/(df['diff_seconds']+0.01)\n",
    "    # 瞬时加速度 m/s2\n",
    "    df['instant_acc']=df['mean_speed']/(df['diff_seconds']+0.01)\n",
    "    df['cusum_speed']=df.groupby('loadingOrder')['speed'].rolling(window=5).mean().reset_index(drop=True)\n",
    "    #获取船航行经度和维度的行驶比例和总航行占比\n",
    "    df['long_gap'] = abs(df['end_longitude']-df['longitude'])\n",
    "    df['lat_gap'] = abs(df['end_latitude']-df['latitude'])\n",
    "    df['start_long_gap'] = abs(df['start_longitude']-df['longitude'])\n",
    "    df['start_lat_gap'] = abs(df['start_latitude']-df['latitude'])\n",
    "    df['start_long_ratio'] = abs(df['longitude']-df['start_longitude']) / abs(df['end_longitude']-df['start_longitude'])\n",
    "    df['start_lat_ratio'] = abs(df['latitude']-df['start_latitude']) / abs(df['end_latitude']-df['start_latitude'])\n",
    "    df['end_long_ratio'] = abs(df['longitude']-df['end_longitude']) / abs(df['end_longitude']-df['start_longitude'])\n",
    "    df['end_lat_ratio'] = abs(df['latitude']-df['end_latitude']) / abs(df['end_latitude']-df['start_latitude'])\n",
    "    #获取总差距\n",
    "    df['all_start_gap'] = df['start_long_gap'] + df['start_lat_gap']\n",
    "    df['all_start_ratio'] = df['all_start_gap'] / (abs(df['end_longitude']-df['start_longitude'])+abs(df['end_latitude']-df['start_latitude']))\n",
    "    df['all_end_gap'] = df['long_gap'] + df['lat_gap']\n",
    "    df['all_end_ratio'] = df['all_end_gap'] / (abs(df['end_longitude']-df['start_longitude'])+abs(df['end_latitude']-df['start_latitude']))\n",
    "    \n",
    "    #获取年月日等时间特征\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['day'] = df['timestamp'].dt.day\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['time'] = df['month'].astype(str)+'-'+df['day'].astype(str)\n",
    "    \n",
    "    ## 得到最早的时间\n",
    "    tmp=df.drop_duplicates('loadingOrder',keep='first').reset_index(drop=True)\n",
    "    tmp=tmp[['loadingOrder','timestamp','direction']]\n",
    "    tmp.columns=['loadingOrder','start_time','start_direction']\n",
    "    df=df.merge(tmp,on='loadingOrder',how='left')\n",
    "    df['have_run_time']=(df['timestamp']-df['start_time']).dt.total_seconds()//3600\n",
    "    df['distanc2taget']=distance(df.latitude.values,df.end_latitude.values,df.longitude.values,df.end_longitude.values)\n",
    "    df['cusum_mean_speed'] = df['cusum_distance']/(df['have_run_time']+0.01)\n",
    "    # 瞬时加速度 m/s2\n",
    "    df['cusum_instant_acc']=df['cusum_mean_speed']/(df['have_run_time']+0.01)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:127: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get train data: (4559407, 58)\n",
      "get test : (34712, 56)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label=get_labe(train_df,feat='end_time')\n",
    "train_data=get_feature(train_df)\n",
    "train_data=train_data.merge(train_label,on='loadingOrder',how='left')\n",
    "train_data['label']=(train_data['end_time']-train_data['timestamp']).dt.total_seconds()//3600\n",
    "print('get train data:',train_data.shape)\n",
    "\n",
    "test = get_feature(test_df)\n",
    "print('get test :',test.shape)\n",
    "\n",
    "train_data,test=type_encoding(train_data,test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TRANSPORT_TRACE', 'carrierName', 'direction', 'latitude', 'longitude', 'speed', 'vesselMMSI', 'start_sport', 'end_sport', 'start_mean_time', 'end_mean_time', 'TRANSPORT_TRACE_mean_time', 'lat_diff', 'lon_diff', 'move_leng', 'cusum_distance', 'cusum_speed', 'mean_speed', 'instant_acc', 'long_gap', 'lat_gap', 'start_long_gap', 'start_lat_gap', 'start_long_ratio', 'start_lat_ratio', 'end_long_ratio', 'end_lat_ratio', 'all_start_gap', 'all_start_ratio', 'all_end_gap', 'all_end_ratio', 'month', 'day', 'hour', 'time', 'start_direction', 'have_run_time', 'distanc2taget', 'cusum_mean_speed', 'cusum_instant_acc'] 40\n"
     ]
    }
   ],
   "source": [
    "features = [c for c in train_data.columns if c not in ['index','loadingOrder', 'label','start_time','end_time',\n",
    "            'geohash_doc','timestamp','isTest', 'end_longitude', 'end_latitude', 'start_longitude', 'start_latitude',\n",
    "                            'anchor', 'stop', 'delay','speed_acc',\n",
    "            'predict','cusum_direction','local_lat_lon', 'direction_diff','speed_diff','direction_valc',\n",
    "                        'gps_label', 'cos_direction','target_erro','diff_seconds','curr_week','curr_hour','leng'\n",
    "                                        \n",
    "]]\n",
    "print(features,len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['direction', 'latitude', 'longitude', 'speed', 'diff_seconds', 'TRANSPORT_TRACE', 'vesselMMSI', 'carrierName', 'start_sport', 'end_sport', 'end_longitude', 'end_latitude', 'start_longitude', 'start_latitude', 'lat_diff', 'lon_diff', 'speed_diff', 'direction_diff', 'anchor', 'stop', 'move_leng', 'cusum_distance', 'cusum_stop', 'direction_valc', 'mean_speed', 'instant_acc', 'long_gap', 'lat_gap', 'start_long_gap', 'start_lat_gap', 'all_start_gap', 'all_end_gap', 'start_direction', 'have_run_time', 'cusum_mean_speed', 'cusum_instant_acc']\n",
      "36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['direction', 'latitude', 'longitude', 'speed', 'diff_seconds', 'TRANSPORT_TRACE', 'vesselMMSI', 'carrierName', 'start_sport', \n",
    " 'end_sport', 'end_longitude', 'end_latitude', 'start_longitude', 'start_latitude', 'lat_diff', 'lon_diff', 'speed_diff', 'direction_diff', 'anchor',\n",
    " 'stop', 'move_leng', 'cusum_distance', 'cusum_stop', 'direction_valc', 'mean_speed', 'instant_acc', 'long_gap', \n",
    " 'lat_gap', 'start_long_gap', 'start_lat_gap', 'all_start_gap', 'all_end_gap', 'start_direction', 'have_run_time', \n",
    " 'cusum_mean_speed', 'cusum_instant_acc','cusum_speed']\n",
    "\n",
    "print(features)\n",
    "print(len(features))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******第1折**********\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l2: 290.398\n",
      "[200]\tvalid_0's l2: 137.217\n",
      "[300]\tvalid_0's l2: 87.9246\n",
      "[400]\tvalid_0's l2: 64.1095\n",
      "[500]\tvalid_0's l2: 50.345\n",
      "[600]\tvalid_0's l2: 39.4002\n",
      "[700]\tvalid_0's l2: 33.6221\n",
      "[800]\tvalid_0's l2: 28.4568\n",
      "[900]\tvalid_0's l2: 24.1927\n",
      "[1000]\tvalid_0's l2: 21.0066\n",
      "[1100]\tvalid_0's l2: 18.4622\n",
      "[1200]\tvalid_0's l2: 16.4171\n",
      "[1300]\tvalid_0's l2: 14.8983\n",
      "[1400]\tvalid_0's l2: 13.5368\n",
      "[1500]\tvalid_0's l2: 12.4583\n",
      "[1600]\tvalid_0's l2: 11.3806\n",
      "[1700]\tvalid_0's l2: 10.54\n",
      "[1800]\tvalid_0's l2: 9.80811\n",
      "[1900]\tvalid_0's l2: 9.17254\n",
      "[2000]\tvalid_0's l2: 8.63606\n",
      "[2100]\tvalid_0's l2: 8.17645\n",
      "[2200]\tvalid_0's l2: 7.72456\n",
      "[2300]\tvalid_0's l2: 7.32529\n",
      "[2400]\tvalid_0's l2: 6.93869\n",
      "[2500]\tvalid_0's l2: 6.64196\n",
      "[2600]\tvalid_0's l2: 6.34846\n",
      "[2700]\tvalid_0's l2: 6.0834\n",
      "[2800]\tvalid_0's l2: 5.8377\n",
      "[2900]\tvalid_0's l2: 5.59295\n",
      "[3000]\tvalid_0's l2: 5.39187\n",
      "[3100]\tvalid_0's l2: 5.19327\n",
      "[3200]\tvalid_0's l2: 5.00739\n",
      "[3300]\tvalid_0's l2: 4.83544\n",
      "[3400]\tvalid_0's l2: 4.67792\n",
      "[3500]\tvalid_0's l2: 4.5194\n",
      "[3600]\tvalid_0's l2: 4.39105\n",
      "[3700]\tvalid_0's l2: 4.2697\n",
      "[3800]\tvalid_0's l2: 4.14735\n",
      "[3900]\tvalid_0's l2: 4.04658\n",
      "[4000]\tvalid_0's l2: 3.938\n",
      "[4100]\tvalid_0's l2: 3.84306\n",
      "[4200]\tvalid_0's l2: 3.74492\n",
      "[4300]\tvalid_0's l2: 3.64715\n",
      "[4400]\tvalid_0's l2: 3.55767\n",
      "[4500]\tvalid_0's l2: 3.48256\n",
      "[4600]\tvalid_0's l2: 3.40745\n",
      "[4700]\tvalid_0's l2: 3.32709\n",
      "[4800]\tvalid_0's l2: 3.25505\n",
      "[4900]\tvalid_0's l2: 3.19105\n",
      "[5000]\tvalid_0's l2: 3.12847\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l2: 3.12847\n",
      "******第2折**********\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l2: 283.876\n",
      "[200]\tvalid_0's l2: 133.599\n",
      "[300]\tvalid_0's l2: 89.2051\n",
      "[400]\tvalid_0's l2: 65.0146\n",
      "[500]\tvalid_0's l2: 50.7562\n",
      "[600]\tvalid_0's l2: 40.3791\n",
      "[700]\tvalid_0's l2: 33.1682\n",
      "[800]\tvalid_0's l2: 27.7553\n",
      "[900]\tvalid_0's l2: 24.0669\n",
      "[1000]\tvalid_0's l2: 20.8454\n",
      "[1100]\tvalid_0's l2: 18.3957\n",
      "[1200]\tvalid_0's l2: 16.5206\n",
      "[1300]\tvalid_0's l2: 14.7898\n",
      "[1400]\tvalid_0's l2: 13.4163\n",
      "[1500]\tvalid_0's l2: 12.2335\n",
      "[1600]\tvalid_0's l2: 11.3486\n",
      "[1700]\tvalid_0's l2: 10.4888\n",
      "[1800]\tvalid_0's l2: 9.75556\n",
      "[1900]\tvalid_0's l2: 9.16256\n",
      "[2000]\tvalid_0's l2: 8.57651\n",
      "[2100]\tvalid_0's l2: 8.11011\n",
      "[2200]\tvalid_0's l2: 7.65771\n",
      "[2300]\tvalid_0's l2: 7.26435\n",
      "[2400]\tvalid_0's l2: 6.93698\n",
      "[2500]\tvalid_0's l2: 6.64191\n",
      "[2600]\tvalid_0's l2: 6.36771\n",
      "[2700]\tvalid_0's l2: 6.11596\n",
      "[2800]\tvalid_0's l2: 5.87902\n",
      "[2900]\tvalid_0's l2: 5.61032\n",
      "[3000]\tvalid_0's l2: 5.39899\n",
      "[3100]\tvalid_0's l2: 5.20512\n",
      "[3200]\tvalid_0's l2: 5.0189\n",
      "[3300]\tvalid_0's l2: 4.84078\n",
      "[3400]\tvalid_0's l2: 4.6797\n",
      "[3500]\tvalid_0's l2: 4.5296\n",
      "[3600]\tvalid_0's l2: 4.39391\n",
      "[3700]\tvalid_0's l2: 4.25754\n",
      "[3800]\tvalid_0's l2: 4.14346\n",
      "[3900]\tvalid_0's l2: 4.02992\n",
      "[4000]\tvalid_0's l2: 3.92055\n",
      "[4100]\tvalid_0's l2: 3.82011\n",
      "[4200]\tvalid_0's l2: 3.72776\n",
      "[4300]\tvalid_0's l2: 3.63855\n",
      "[4400]\tvalid_0's l2: 3.55575\n",
      "[4500]\tvalid_0's l2: 3.4703\n",
      "[4600]\tvalid_0's l2: 3.38894\n",
      "[4700]\tvalid_0's l2: 3.3131\n",
      "[4800]\tvalid_0's l2: 3.24166\n",
      "[4900]\tvalid_0's l2: 3.16989\n",
      "[5000]\tvalid_0's l2: 3.10277\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l2: 3.10277\n",
      "******第3折**********\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l2: 287.624\n",
      "[200]\tvalid_0's l2: 133.911\n",
      "[300]\tvalid_0's l2: 91.9982\n",
      "[400]\tvalid_0's l2: 66.6867\n",
      "[500]\tvalid_0's l2: 51.2124\n",
      "[600]\tvalid_0's l2: 40.3757\n",
      "[700]\tvalid_0's l2: 33.1049\n",
      "[800]\tvalid_0's l2: 28.1153\n",
      "[900]\tvalid_0's l2: 24.1787\n",
      "[1000]\tvalid_0's l2: 21.1462\n",
      "[1100]\tvalid_0's l2: 18.7117\n",
      "[1200]\tvalid_0's l2: 16.7777\n",
      "[1300]\tvalid_0's l2: 15.0558\n",
      "[1400]\tvalid_0's l2: 13.6815\n",
      "[1500]\tvalid_0's l2: 12.5416\n",
      "[1600]\tvalid_0's l2: 11.5858\n",
      "[1700]\tvalid_0's l2: 10.7791\n",
      "[1800]\tvalid_0's l2: 10.107\n",
      "[1900]\tvalid_0's l2: 9.48452\n",
      "[2000]\tvalid_0's l2: 8.87156\n",
      "[2100]\tvalid_0's l2: 8.32714\n",
      "[2200]\tvalid_0's l2: 7.86933\n",
      "[2300]\tvalid_0's l2: 7.45659\n",
      "[2400]\tvalid_0's l2: 7.08588\n",
      "[2500]\tvalid_0's l2: 6.74136\n",
      "[2600]\tvalid_0's l2: 6.42224\n",
      "[2700]\tvalid_0's l2: 6.12895\n",
      "[2800]\tvalid_0's l2: 5.88645\n",
      "[2900]\tvalid_0's l2: 5.65331\n",
      "[3000]\tvalid_0's l2: 5.44296\n",
      "[3100]\tvalid_0's l2: 5.24375\n",
      "[3200]\tvalid_0's l2: 5.05899\n",
      "[3300]\tvalid_0's l2: 4.88954\n",
      "[3400]\tvalid_0's l2: 4.72596\n",
      "[3500]\tvalid_0's l2: 4.58882\n",
      "[3600]\tvalid_0's l2: 4.45574\n",
      "[3700]\tvalid_0's l2: 4.3224\n",
      "[3800]\tvalid_0's l2: 4.20734\n",
      "[3900]\tvalid_0's l2: 4.09965\n",
      "[4000]\tvalid_0's l2: 3.99822\n",
      "[4100]\tvalid_0's l2: 3.89662\n",
      "[4200]\tvalid_0's l2: 3.79954\n",
      "[4300]\tvalid_0's l2: 3.70488\n",
      "[4400]\tvalid_0's l2: 3.62309\n",
      "[4500]\tvalid_0's l2: 3.5474\n",
      "[4600]\tvalid_0's l2: 3.47112\n",
      "[4700]\tvalid_0's l2: 3.3886\n",
      "[4800]\tvalid_0's l2: 3.31856\n",
      "[4900]\tvalid_0's l2: 3.25125\n",
      "[5000]\tvalid_0's l2: 3.18144\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l2: 3.18144\n",
      "******第4折**********\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l2: 286.007\n",
      "[200]\tvalid_0's l2: 133.107\n",
      "[300]\tvalid_0's l2: 87.5459\n",
      "[400]\tvalid_0's l2: 63.8807\n",
      "[500]\tvalid_0's l2: 49.9241\n",
      "[600]\tvalid_0's l2: 39.8875\n",
      "[700]\tvalid_0's l2: 33.1632\n",
      "[800]\tvalid_0's l2: 27.5528\n",
      "[900]\tvalid_0's l2: 23.7017\n",
      "[1000]\tvalid_0's l2: 20.787\n",
      "[1100]\tvalid_0's l2: 18.4505\n",
      "[1200]\tvalid_0's l2: 16.5389\n",
      "[1300]\tvalid_0's l2: 14.8051\n",
      "[1400]\tvalid_0's l2: 13.4536\n",
      "[1500]\tvalid_0's l2: 12.3452\n",
      "[1600]\tvalid_0's l2: 11.372\n",
      "[1700]\tvalid_0's l2: 10.5386\n",
      "[1800]\tvalid_0's l2: 9.81393\n",
      "[1900]\tvalid_0's l2: 9.20489\n",
      "[2000]\tvalid_0's l2: 8.66009\n",
      "[2100]\tvalid_0's l2: 8.17839\n",
      "[2200]\tvalid_0's l2: 7.75437\n",
      "[2300]\tvalid_0's l2: 7.36379\n",
      "[2400]\tvalid_0's l2: 7.01829\n",
      "[2500]\tvalid_0's l2: 6.69662\n",
      "[2600]\tvalid_0's l2: 6.41177\n",
      "[2700]\tvalid_0's l2: 6.12425\n",
      "[2800]\tvalid_0's l2: 5.88539\n",
      "[2900]\tvalid_0's l2: 5.65309\n",
      "[3000]\tvalid_0's l2: 5.44303\n",
      "[3100]\tvalid_0's l2: 5.25627\n",
      "[3200]\tvalid_0's l2: 5.07381\n",
      "[3300]\tvalid_0's l2: 4.89842\n",
      "[3400]\tvalid_0's l2: 4.7493\n",
      "[3500]\tvalid_0's l2: 4.59875\n",
      "[3600]\tvalid_0's l2: 4.46749\n",
      "[3700]\tvalid_0's l2: 4.33584\n",
      "[3800]\tvalid_0's l2: 4.22089\n",
      "[3900]\tvalid_0's l2: 4.0981\n",
      "[4000]\tvalid_0's l2: 3.99908\n",
      "[4100]\tvalid_0's l2: 3.90295\n",
      "[4200]\tvalid_0's l2: 3.81129\n",
      "[4300]\tvalid_0's l2: 3.72651\n",
      "[4400]\tvalid_0's l2: 3.65031\n",
      "[4500]\tvalid_0's l2: 3.56309\n",
      "[4600]\tvalid_0's l2: 3.484\n",
      "[4700]\tvalid_0's l2: 3.40296\n",
      "[4800]\tvalid_0's l2: 3.33137\n",
      "[4900]\tvalid_0's l2: 3.25699\n",
      "[5000]\tvalid_0's l2: 3.1851\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l2: 3.1851\n",
      "******第5折**********\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l2: 289.518\n",
      "[200]\tvalid_0's l2: 130.946\n",
      "[300]\tvalid_0's l2: 87.8036\n",
      "[400]\tvalid_0's l2: 64.886\n",
      "[500]\tvalid_0's l2: 50.5903\n",
      "[600]\tvalid_0's l2: 39.7971\n",
      "[700]\tvalid_0's l2: 32.8858\n",
      "[800]\tvalid_0's l2: 27.6308\n",
      "[900]\tvalid_0's l2: 23.8478\n",
      "[1000]\tvalid_0's l2: 20.6081\n",
      "[1100]\tvalid_0's l2: 18.3051\n",
      "[1200]\tvalid_0's l2: 16.3419\n",
      "[1300]\tvalid_0's l2: 14.6938\n",
      "[1400]\tvalid_0's l2: 13.4204\n",
      "[1500]\tvalid_0's l2: 12.3427\n",
      "[1600]\tvalid_0's l2: 11.4104\n",
      "[1700]\tvalid_0's l2: 10.5339\n",
      "[1800]\tvalid_0's l2: 9.81829\n",
      "[1900]\tvalid_0's l2: 9.21568\n",
      "[2000]\tvalid_0's l2: 8.66309\n",
      "[2100]\tvalid_0's l2: 8.15308\n",
      "[2200]\tvalid_0's l2: 7.7111\n",
      "[2300]\tvalid_0's l2: 7.32391\n",
      "[2400]\tvalid_0's l2: 6.96278\n",
      "[2500]\tvalid_0's l2: 6.62306\n",
      "[2600]\tvalid_0's l2: 6.32076\n",
      "[2700]\tvalid_0's l2: 6.03513\n",
      "[2800]\tvalid_0's l2: 5.79606\n",
      "[2900]\tvalid_0's l2: 5.58065\n",
      "[3000]\tvalid_0's l2: 5.35473\n",
      "[3100]\tvalid_0's l2: 5.15337\n",
      "[3200]\tvalid_0's l2: 4.96861\n",
      "[3300]\tvalid_0's l2: 4.81853\n",
      "[3400]\tvalid_0's l2: 4.66948\n",
      "[3500]\tvalid_0's l2: 4.52475\n",
      "[3600]\tvalid_0's l2: 4.39656\n",
      "[3700]\tvalid_0's l2: 4.26788\n",
      "[3800]\tvalid_0's l2: 4.14016\n",
      "[3900]\tvalid_0's l2: 4.03439\n",
      "[4000]\tvalid_0's l2: 3.93254\n",
      "[4100]\tvalid_0's l2: 3.83479\n",
      "[4200]\tvalid_0's l2: 3.74218\n",
      "[4300]\tvalid_0's l2: 3.65065\n",
      "[4400]\tvalid_0's l2: 3.56101\n",
      "[4500]\tvalid_0's l2: 3.4789\n",
      "[4600]\tvalid_0's l2: 3.40405\n",
      "[4700]\tvalid_0's l2: 3.33013\n",
      "[4800]\tvalid_0's l2: 3.25754\n",
      "[4900]\tvalid_0's l2: 3.19662\n",
      "[5000]\tvalid_0's l2: 3.13328\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's l2: 3.13328\n",
      "mean_squared_error: 3.146212805040218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,explained_variance_score\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "from  lightgbm.sklearn import LGBMRegressor\n",
    "def build_model(train_data, test, pred, label, seed=1080, is_shuffle=True):\n",
    "    train_pred = np.zeros((train_data.shape[0], ))\n",
    "    test_pred = np.zeros((test.shape[0], ))\n",
    "    n_splits = 5\n",
    "    # Kfold\n",
    "    fold = KFold(n_splits=n_splits, shuffle=is_shuffle, random_state=seed)\n",
    "    kf_way = fold.split(train_data[pred])\n",
    "    # params\n",
    "#     test_x=np.concatenate([test[pred].values,geohash_test],axis=1)\n",
    "    # train\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(kf_way, start=1):\n",
    "        print('******第{}折**********'.format(n_fold))\n",
    "        train_x, train_y = train_data[pred].iloc[train_idx].values, train_data[label].iloc[train_idx]\n",
    "        valid_x, valid_y = train_data[pred].iloc[valid_idx].values, train_data[label].iloc[valid_idx]\n",
    "#         geohash_tr_x,geohash_val_x=geohash_train[train_idx],geohash_train[valid_idx]\n",
    "#         train_x=np.concatenate([train_x,geohash_tr_x],axis=1)\n",
    "#         valid_x=np.concatenate([valid_x,geohash_val_x],axis=1)\n",
    "        \n",
    "        # 数据加载\n",
    "        clf=LGBMRegressor( learning_rate=0.05,\n",
    "        n_estimators=5000,\n",
    "        num_leaves=156,\n",
    "        subsample=0.8,\n",
    "        njobs=-1,\n",
    "        max_depth=6,\n",
    "        reg_lambda=0,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=1090,  # 2019\n",
    "        metric=['mse'])\n",
    "        \n",
    "        clf.fit(\n",
    "        train_x, train_y,\n",
    "        eval_set=[(valid_x, valid_y)],\n",
    "        eval_metric=['mse'],\n",
    "        categorical_feature='auto',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=100)        \n",
    "        \n",
    "        train_pred[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration_)\n",
    " \n",
    "        test_pred += clf.predict(test[pred], num_iteration=clf.best_iteration_)/fold.n_splits\n",
    "    train_data['predict']=train_pred\n",
    "    print('mean_squared_error:',mean_squared_error(train_data[label].values,train_pred))\n",
    "    test['label'] = test_pred\n",
    "    return test[['loadingOrder', 'label']],clf\n",
    "\n",
    "\n",
    "result,clf = build_model(train_data, test,pred= features,label= 'label', is_shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3419555, 36)\n",
      "(1139852, 36)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l2: 3869.89\n",
      "[200]\tvalid_0's l2: 2148.24\n",
      "[300]\tvalid_0's l2: 1454.68\n",
      "[400]\tvalid_0's l2: 1070.06\n",
      "[500]\tvalid_0's l2: 812.235\n",
      "[600]\tvalid_0's l2: 642.752\n",
      "[700]\tvalid_0's l2: 524.619\n",
      "[800]\tvalid_0's l2: 442.545\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-4f56d71359e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m verbose=100)        \n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    736\u001b[0m                                        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                                        \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                                        callbacks=callbacks)\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    593\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1924\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1925\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1926\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1927\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,explained_variance_score\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "from  lightgbm.sklearn import LGBMRegressor\n",
    "train_x,valid_x, train_y, valid_y=train_test_split(train_data[features].values,train_data['label'].values,test_size=0.25,random_state=1068)\n",
    "print(train_x.shape)\n",
    "print(valid_x.shape)\n",
    " # 数据加载\n",
    "clf=LGBMRegressor( learning_rate=0.05,\n",
    "n_estimators=5500,\n",
    "num_leaves=156,\n",
    "subsample=0.8,\n",
    "njobs=-1,\n",
    "max_depth=6,\n",
    "reg_lambda=0.5,\n",
    "colsample_bytree=0.8,\n",
    "random_state=1090,  # 2019\n",
    "metric=['mse'])\n",
    "\n",
    "clf.fit(\n",
    "train_x, train_y,\n",
    "eval_set=[(valid_x, valid_y)],\n",
    "eval_metric=['mse'],\n",
    "categorical_feature='auto',\n",
    "early_stopping_rounds=50,\n",
    "verbose=100)        \n",
    "\n",
    "test_pred = clf.predict(test[features], num_iteration=clf.best_iteration_)\n",
    "# train_data['predict']=train_pred\n",
    "# print('mean_squared_error:',mean_squared_error(valid_y,train_pred))\n",
    "test['label'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vesselMMSI', 22410),\n",
       " ('start_direction', 21916),\n",
       " ('start_longitude', 21052),\n",
       " ('cusum_mean_speed', 20447),\n",
       " ('start_latitude', 20155),\n",
       " ('end_latitude', 19405),\n",
       " ('end_longitude', 17286),\n",
       " ('long_gap', 16044),\n",
       " ('TRANSPORT_TRACE', 16041),\n",
       " ('cusum_instant_acc', 15206),\n",
       " ('have_run_time', 14519),\n",
       " ('all_end_gap', 13953),\n",
       " ('end_sport', 12783),\n",
       " ('cusum_distance', 12754),\n",
       " ('lat_gap', 12530),\n",
       " ('latitude', 11852),\n",
       " ('longitude', 9118),\n",
       " ('start_lat_gap', 9067),\n",
       " ('start_long_gap', 8989),\n",
       " ('all_start_gap', 7901),\n",
       " ('speed', 7554),\n",
       " ('direction', 5730),\n",
       " ('lat_diff', 3905),\n",
       " ('start_sport', 3654),\n",
       " ('lon_diff', 2795),\n",
       " ('mean_speed', 2626),\n",
       " ('move_leng', 1137),\n",
       " ('instant_acc', 1018),\n",
       " ('diff_seconds', 970),\n",
       " ('speed_diff', 615),\n",
       " ('direction_diff', 329)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_dict={features[i]: f for i,f in enumerate(clf.feature_importances_)}\n",
    "# import_dict.update({ i:v for i,v in enumerate(clf.feature_importances_[60:])})\n",
    "sorted(import_dict.items(),key=lambda x:x[1],reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result=test.loc[test.have_run_time!=0]\n",
    "result=test.copy()\n",
    "# result['diff_label']=result.groupby('loadingOrder')['label'].diff(1)\n",
    "# result1=result.loc[(result.diff_label>=0)]\n",
    "# result=pd.concat([result1,result.loc[~(result.loadingOrder.isin(result1.loadingOrder))]],axis=0)\n",
    "\n",
    "# result=test.drop_duplicates('loadingOrder','last')\n",
    "\n",
    "result['ETA']=(result['timestamp']+result['label'].apply(lambda x:pd.Timedelta(hours=x)))\n",
    "result['avg_time']=(result['ETA']-result['onboardDate']).dt.total_seconds()//3600\n",
    "\n",
    "# #-----------\n",
    "# # 箱型去除异常算法\n",
    "# result['q1']=result.groupby('loadingOrder')['avg_time'].transform(lambda x:x.quantile(0.9))# q1.columns=['loadingOrder','q1']\n",
    "# result['q2']=result.groupby('loadingOrder')['avg_time'].transform(lambda x:x.quantile(0.2))\n",
    "# result['q1_bool']=(result['avg_time']>result['q1']).astype('int')\n",
    "# result['q2_bool']=(result['avg_time']<result['q2']).astype('int')\n",
    "# result['avg_time']=result['q1_bool']*result['q1']+result['avg_time']*(-result['q1_bool']+1)\n",
    "# result['avg_time']=result['q2_bool']*result['q2']+result['avg_time']*(-result['q2_bool']+1)\n",
    "\n",
    "result=result.groupby('loadingOrder')['avg_time'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs=test_df\n",
    "subs=subs.merge(result[['loadingOrder','avg_time']],on='loadingOrder',how='left')\n",
    "subs['ETA']=(subs['onboardDate']+subs['avg_time'].apply(lambda x:pd.Timedelta(hours=x))).apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "# del subs['label'],subs['start_time']\n",
    "\n",
    "# subs = test_df.merge(subs, on='loadingOrder', how='left')\n",
    "subs['onboardDate'] = test_df['onboardDate'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "subs['creatDate'] = pd.datetime.now().strftime('%Y/%m/%d  %H:%M:%S')\n",
    "subs['timestamp'] =test_df['timestamp'].apply(lambda x:x.strftime('%Y-%m-%dT%H:%M:%S.000Z'))\n",
    "# 整理columns顺序\n",
    "subs =subs[['loadingOrder', 'timestamp', 'longitude', 'latitude', 'carrierName', 'vesselMMSI', 'onboardDate', 'ETA', 'creatDate']]\n",
    "subs.to_csv('submitte.csv',index=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.639749942383038"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs=test_df\n",
    "subs=subs.merge(result[['loadingOrder','avg_time']],on='loadingOrder',how='left')\n",
    "subs['ETA']=(subs['onboardDate']+subs['avg_time'].apply(lambda x:pd.Timedelta(hours=x))).apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "subs['onboardDate'] = test_df['onboardDate'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "subs['creatDate'] = pd.datetime.now().strftime('%Y/%m/%d  %H:%M:%S')\n",
    "subs['timestamp'] =test_df['timestamp'].apply(lambda x:x.strftime('%Y-%m-%dT%H:%M:%S.000Z'))\n",
    "# 整理columns顺序\n",
    "subs =subs[['loadingOrder', 'timestamp', 'longitude', 'latitude', 'carrierName', 'vesselMMSI', 'onboardDate', 'ETA', 'creatDate']]\n",
    "subs.to_csv('submmite.csv',index=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多模多种子 多参数多分数 融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2['onboardDate'] = pd.to_datetime(result2['onboardDate'])\n",
    "result2['onboardDate'] = result2['onboardDate'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "result2['ETA'] = pd.to_datetime(result2['ETA'])\n",
    "result2['ETA'] = result2['ETA'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "result2['creatDate'] = pd.to_datetime(result2['creatDate'])\n",
    "result2['creatDate'] = result2['creatDate'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1['timestamp'] = pd.to_datetime(result1['timestamp'])\n",
    "result1['timestamp'] = result1['timestamp'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "result1['base_time'] = '2019/01/01  00:00:00'\n",
    "result1['base_time'] = pd.to_datetime(result1['base_time'])\n",
    "result1['ETA'] = pd.to_datetime(result1['ETA'])\n",
    "result1['time_gap'] = (result1['ETA'] - result1['base_time']).dt.total_seconds()\n",
    "result1['gap'] = result1.groupby('loadingOrder')['time_gap'].transform('median')\n",
    "result1['ETA1'] = (result1['base_time']+result1['gap'].apply(lambda x:pd.Timedelta(seconds=x))).apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2['timestamp'] = pd.to_datetime(result2['timestamp'])\n",
    "result2['timestamp'] = result2['timestamp'].apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "result2['base_time'] = '2019/01/01  00:00:00'\n",
    "result2['base_time'] = pd.to_datetime(result1['base_time'])\n",
    "result2['ETA'] = pd.to_datetime(result2['ETA'])\n",
    "result2['time_gap'] = (result2['ETA'] - result2['base_time']).dt.total_seconds()\n",
    "result2['gap'] = result2.groupby('loadingOrder')['time_gap'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#求均值\n",
    "result1['gap1'] = result2['gap']\n",
    "result1['gap2'] = (result1['gap'] + result1['gap1'])/2\n",
    "\n",
    "result1['ETA2'] = (result1['base_time']+result1['gap2'].apply(lambda x:pd.Timedelta(seconds=x))).apply(lambda x:x.strftime('%Y/%m/%d  %H:%M:%S'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
